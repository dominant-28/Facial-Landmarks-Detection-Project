{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FACIAL LANDMARKS DETECTION PROJECT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTING ALL DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from xml.etree import ElementTree as ET\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from skimage import io\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.transforms.functional as TF\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CLASS FOR THE AUGMENTATION OF IMAGES**\n",
    "----- **INCLUDED METHODS**\n",
    "* OFFSET CROP\n",
    "* RANDOM FACECROP\n",
    "* RANDOM ROTATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandmarksAugmentation:\n",
    "    def __init__(self, image_dim, brightness, contrast, saturation, hue, face_offset, crop_offset, rotation_limit):\n",
    "        # Initialization parameters\n",
    "        self.image_dim = image_dim            #Target dimension for the image.\n",
    "        self.face_offset = face_offset        #Padding around the detected face during cropping.\n",
    "        self.crop_offset = crop_offset        #Additional resizing offset.\n",
    "        self.rotation_limit = rotation_limit  #Maximum rotation angle for random rotation.\n",
    "\n",
    "        '''Initializes a color jitter transformation using transforms.ColorJitter to adjust the brightness, contrast,\n",
    "        saturation, and hue of the image.'''\n",
    "\n",
    "        self.transform = transforms.ColorJitter(brightness, contrast, saturation, hue)\n",
    "\n",
    "    def offset_crop(self, image, landmarks, crops_coordinates):\n",
    "\n",
    "        ''' This method crops the image based on given facial crop coordinates (crops_coordinates),\n",
    "        modifies the facial landmarks accordingly, and resizes the image.'''\n",
    "\n",
    "\n",
    "        '''Extracts the left, top, width, and height values from crops_coordinates,\n",
    "        adding or subtracting face_offset to include padding around the face.'''\n",
    "\n",
    "        left = int(crops_coordinates['left']) - self.face_offset\n",
    "        top = int(crops_coordinates['top']) - self.face_offset\n",
    "        width = int(crops_coordinates['width']) + (2 * self.face_offset)\n",
    "        height = int(crops_coordinates['height']) + (2 * self.face_offset)\n",
    "\n",
    "        image = TF.crop(image, top, left, height, width)\n",
    "        landmarks = landmarks - np.array([[left, top]])  #making the landmarks relative to this new cropped image's coordinate system.\n",
    "\n",
    "        # Adjusting the dimension of the cropped image\n",
    "        new_dim = self.image_dim + self.crop_offset\n",
    "        image = TF.resize(image, (new_dim, new_dim))\n",
    "\n",
    "        # Scaling landmarks according to the new dimensions\n",
    "        #This adjustment is necessary to maintain the correct placement of landmarks after the image has been resized from its original dimensions.\n",
    "        landmarks[:, 0] *= new_dim / width\n",
    "        landmarks[:, 1] *= new_dim / height\n",
    "\n",
    "        return image, landmarks\n",
    "\n",
    "    def random_face_crop(self, image, landmarks):\n",
    "        # Converting the image to a NumPy array for manipulation\n",
    "        image = np.array(image)\n",
    "\n",
    "        # Get the height and width of the image\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        # Randomly selecting the top-left corner for the crop,Ensuring the crop is within the limit\n",
    "        top = np.random.randint(0, h - self.image_dim)\n",
    "        left = np.random.randint(0, w - self.image_dim)\n",
    "\n",
    "        # Cropping the image\n",
    "        image = image[top: top + self.image_dim, left: left + self.image_dim]\n",
    "        landmarks = landmarks - np.array([[left, top]])\n",
    "\n",
    "        # Returning as PIL image and updated landmarks\n",
    "        return TF.to_pil_image(image), landmarks\n",
    "\n",
    "    def random_rotation(self, image, landmarks):\n",
    "        # Generating a random rotation angle\n",
    "        angle = np.random.uniform(-self.rotation_limit, self.rotation_limit)\n",
    "\n",
    "        # Creating a rotation transformation matrix\n",
    "        landmarks_transformation = np.array([\n",
    "            [+np.cos(np.radians(angle)), -np.sin(np.radians(angle))],\n",
    "            [+np.sin(np.radians(angle)), +np.cos(np.radians(angle))]\n",
    "        ])\n",
    "\n",
    "        # Rotating the image by the generated angle\n",
    "        image = TF.rotate(image, angle)\n",
    "\n",
    "        # Transforming landmarks by the rotation matrix\n",
    "        '''Assuming the landmark coordinates are normalized (i.e., they range from 0 to 1),\n",
    "        subtracting 0.5 will change the coordinate system to be centered around (0.5, 0.5), which is the center of the image in normalized coordinates.'''\n",
    "\n",
    "        landmarks = landmarks - 0.5  #This adjusts the landmarks by shifting them so that the origin (0, 0) is at the center of the coordinate system. This is done to apply the rotation around the center of the landmarks.\n",
    "        transformed_landmarks = np.matmul(landmarks, landmarks_transformation)\n",
    "        transformed_landmarks = transformed_landmarks + 0.5  # Re-centering after transformation\n",
    "\n",
    "        return image, transformed_landmarks\n",
    "\n",
    "    def __call__(self, image, landmarks, crops_coordinates):\n",
    "        # Apply offset cropping first\n",
    "        image, landmarks = self.offset_crop(image, landmarks, crops_coordinates)\n",
    "\n",
    "        # Apply random cropping after offset cropping\n",
    "        image, landmarks = self.random_face_crop(image, landmarks)\n",
    "\n",
    "        # Apply random rotation on the cropped image and landmarks\n",
    "        image, landmarks = self.random_rotation(image, landmarks)\n",
    "\n",
    "        # Apply color jitter transformation\n",
    "        return self.transform(image), landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PREPROCESSOR CLASS TO PREPROCESS ALL THE IMAGES IN THE DATASET USING DEFINED METHODS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self,\n",
    "                 image_dim,\n",
    "                 brightness,\n",
    "                 contrast,\n",
    "                 saturation,\n",
    "                 hue,\n",
    "                 rotation_limit,\n",
    "                 face_offset,\n",
    "                 crop_offset):\n",
    "\n",
    "        self.image_dim = image_dim\n",
    "\n",
    "        # Initialize the combined FaceLandmarksAugmentation class\n",
    "\n",
    "        self.face_landmarks_augmentation = FaceLandmarksAugmentation(\n",
    "            image_dim=image_dim,\n",
    "            brightness=brightness,\n",
    "            contrast=contrast,\n",
    "            saturation=saturation,\n",
    "            hue=hue,\n",
    "            face_offset=face_offset,\n",
    "            crop_offset=crop_offset,\n",
    "            rotation_limit=rotation_limit\n",
    "        )\n",
    "\n",
    "    def __call__(self, image, landmarks, crops_coordinates):\n",
    "        # Convert image to PIL if it's in NumPy format\n",
    "        image = TF.to_pil_image(image)\n",
    "\n",
    "        # Perform the full augmentation (face cropping, random cropping, random rotation, color jitter)\n",
    "        image, landmarks = self.face_landmarks_augmentation.offset_crop(image, landmarks, crops_coordinates)\n",
    "        image, landmarks = self.face_landmarks_augmentation.random_face_crop(image, landmarks)\n",
    "\n",
    "\n",
    "        # Normalize landmarks by the image size (scaling them to be relative to the image dimensions)\n",
    "        landmarks = landmarks / np.array([*image.size])\n",
    "        image, landmarks = self.face_landmarks_augmentation.random_rotation(image, landmarks)\n",
    "\n",
    "        # Convert the image to grayscale\n",
    "        image = TF.to_grayscale(image)\n",
    "\n",
    "        # Convert the image to a tensor\n",
    "        image = TF.to_tensor(image)\n",
    "\n",
    "        # Normalize the image to the range [-1, 1]\n",
    "        image = (image - image.min()) / (image.max() - image.min())  # Normalize to [0, 1]\n",
    "        image = (2 * image) - 1  # Scale to [-1, 1],This is a common practice in deep learning to center the data around zero and improve model performance.\n",
    "\n",
    "\n",
    "        return image, torch.FloatTensor(landmarks.reshape(-1) - 0.5)   #shifts the coordinates to be centered around zero. This adjustment ensures that landmarks are in the range [-0.5, 0.5], which can help with model convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CREATING THE DATASET CLASS FOR LOADING ALL DATA AND SIMULTANEOUSLY PREPROCESSING THEM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetlandmark(Dataset):\n",
    "  def __init__(self,Preprocessor,train):\n",
    "     self.rootdir= r'C:\\Users\\soham\\OneDrive\\Documents\\ALL TASK OF FACIAL LANDMARK PROJECT\\Facial_Landmark_Detection_project\\archive (1)\\ibug_300W_large_face_landmark_dataset'\n",
    "     self.imagepath=[]\n",
    "     self.landmarks=[]\n",
    "     self.crops_coordinates=[]\n",
    "     self.preprocessor=Preprocessor\n",
    "     self.train=train\n",
    "\n",
    "     '''Parsing the xml tree to extract the data from the dataset'''\n",
    "\n",
    "     tree=ET.parse(os.path.join(self.rootdir,f'labels_ibug_300W_{\"train\" if train else \"test\"}.xml'))\n",
    "     root=tree.getroot()\n",
    "     for child in root[2]:\n",
    "       self.imagepath.append(os.path.join(self.rootdir,child.attrib['file']))\n",
    "       self.crops_coordinates.append(child[0].attrib)\n",
    "       landmark=[]\n",
    "       for i in range(68):\n",
    "          x_coordinate=int(child[0][i].attrib['x'])\n",
    "          y_coordinate=int(child[0][i].attrib['y'])\n",
    "          landmark.append([x_coordinate,y_coordinate])\n",
    "       self.landmarks.append(landmark)\n",
    "     self.landmarks=np.array(self.landmarks).astype('float32')\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imagepath)\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "    image=io.imread(self.imagepath[index],as_gray=False)\n",
    "    landmarks=self.landmarks[index]\n",
    "    crops_coordinates=self.crops_coordinates[index]\n",
    "\n",
    "    image,landmarks=self.preprocessor(image,landmarks,crops_coordinates)\n",
    "\n",
    "    return image,landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **FUNCTION TO SHOW THE IMAGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageshowing(image,landmarks):\n",
    "  plt.figure(figsize=(5,5))\n",
    "  image=(image-image.min())/(image.max()-image.min())  #Normalizes the image to have pixel values in the range [0, 1].\n",
    "  landmarks=landmarks.view(-1,2)\n",
    "  landmarks =(landmarks+0.5)*preprocessor.image_dim  #The landmarks were previously normalized to be in the range [-0.5, 0.5] in the preprocessing step. Adding 0.5 brings them to the range [0, 1].\n",
    "  plt.imshow(image[0],cmap='gray')\n",
    "  plt.scatter(landmarks[:,0],landmarks[:,1],s=20,c='dodgerblue')  #Plots the landmarks on top of the image using plt.scatter().\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GETTING THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor=Preprocessor(\n",
    "                 image_dim=128,\n",
    "                 brightness=0.24,\n",
    "                 contrast=0.15,\n",
    "                 saturation=0.3,\n",
    "                 hue=0.1,\n",
    "                 rotation_limit=14,\n",
    "                 face_offset=32,\n",
    "                 crop_offset=16)\n",
    "trainimages=datasetlandmark(preprocessor,train=True)\n",
    "testimages=datasetlandmark(preprocessor,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1,landmarks1=trainimages[190]\n",
    "imageshowing(image1,landmarks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images = len(trainimages)\n",
    "lenval =int(0.1*total_images)\n",
    "lentrain=total_images-lenval\n",
    "print(f'{lentrain} images for training and {lenval} images for validation')\n",
    "print(f'{len(testimages)} images for the testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SPILTING THE DATASET INTO THE BATCHES AND FUNTION TO VISIUALIZE THE BATCHES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the training dataset into training and validation sets\n",
    "trainimages,valimages=random_split(trainimages,[lentrain,lenval])\n",
    "# Set the batch size\n",
    "batchsize=32\n",
    "# Create DataLoader objects for training, validation, and testing datasets\n",
    "train_data=DataLoader(trainimages,batchsize,True)\n",
    "val_data=DataLoader(valimages,2*batchsize,False)\n",
    "test_data=DataLoader(testimages,2*batchsize,False)\n",
    "\n",
    "# Function to visualize a batch of images along with their landmarks\n",
    "\n",
    "def visiualizebatch(imageslist,landmarkslist,size=14,shape=(4,4)):\n",
    " \n",
    " # Create a figure with a specified size\n",
    "  fig=plt.figure(figsize=(size,size))\n",
    "  # Create a grid layout for the images (4x4 by default)\n",
    "  grid=ImageGrid(fig,111,nrows_ncols=shape,axes_pad=0.08)\n",
    "  # Loop over grid, images, and corresponding landmarks\n",
    "  for i,image,landmarks in zip(grid,imageslist,landmarkslist):\n",
    "    \n",
    "    # Normalize the image to the range [0, 1]\n",
    "    image=(image-image.min())/(image.max()-image.min())\n",
    "    # Reshape landmarks and adjust to image dimensions\n",
    "    landmarks=landmarks.view(-1,2)\n",
    "    landmarks =(landmarks+0.5)*preprocessor.image_dim\n",
    "    landmarks=landmarks.numpy().tolist()\n",
    "    landmarks=np.array([(x,y) for (x,y) in landmarks if 0<=x<=preprocessor.image_dim and 0<=y<=preprocessor.image_dim])\n",
    "    # Plot the image and its landmarks\n",
    "    i.imshow(image[0],cmap='gray')\n",
    "    i.scatter(landmarks[:,0],landmarks[:,1],s=10,c='dodgerblue')\n",
    "    i.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_data:\n",
    "  break\n",
    "visiualizebatch(x[:16],y[:16],shape=(4,4),size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **XECEPTION NETWORK**\n",
    "- ENTRY BLOCK\n",
    "- MIDDLE BLOCK\n",
    "- EXIT BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class depthwiseseperableconv2d(nn.Module):\n",
    "  def __init__(self,input_channels,output_channels,kernel_size,**kwargs):\n",
    "    super(depthwiseseperableconv2d,self).__init__()\n",
    "\n",
    "\n",
    "# Depthwise convolution: Applies a separate convolutional filter to each input channel\n",
    "# 'groups=input_channels' means each channel is convolved independently (depthwise).\n",
    "    self.depthwise=nn.Conv2d(input_channels,input_channels,kernel_size,groups=input_channels,bias=False,**kwargs)\n",
    "\n",
    "# Pointwise convolution: Combines the output of the depthwise layer, \n",
    "# by applying a 1x1 convolution to reduce or increase the number of channels.\n",
    "\n",
    "    self.pointwise=nn.Conv2d(input_channels,output_channels,1,bias=False)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    x=self.depthwise(x)\n",
    "    x=self.pointwise(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class entryblock(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(entryblock,self).__init__()\n",
    "# First convolution block: 1 input channel to 32 channels, followed by BatchNorm and LeakyReLU\n",
    "    self.conv1= nn.Sequential(\n",
    "        nn.Conv2d(1,32,3,padding=1,bias=False),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(0.2)\n",
    "    )\n",
    "# Second convolution block: 32 input channels to 64 channels\n",
    "    self.conv2=nn.Sequential(\n",
    "        nn.Conv2d(32,64,3,padding=1,bias=False),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2)\n",
    "    )\n",
    " # Third block (Residual + Direct Path)\n",
    "    self.conv3_residual=nn.Sequential(\n",
    "        depthwiseseperableconv2d(64,64,3,padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        depthwiseseperableconv2d(64,128,3,padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.MaxPool2d(3,stride=2,padding=1)\n",
    "    )\n",
    "\n",
    "# Direct path: 1x1 convolution to match the output of residual (input 64 channels, output 128 channels), and downsample using stride=2\n",
    "\n",
    "    self.conv3direct=nn.Sequential(\n",
    "        nn.Conv2d(64,128,1,stride=2),\n",
    "        nn.BatchNorm2d(128)\n",
    "    )\n",
    "# Fourth block (Residual + Direct Path)\n",
    "    self.conv4_residual=nn.Sequential(\n",
    "        depthwiseseperableconv2d(128,128,3,padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        depthwiseseperableconv2d(128,256,3,padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.MaxPool2d(3,stride=2,padding=1)\n",
    "    )\n",
    "# Direct path: 1x1 convolution to match the output of residual (input 128 channels, output 256 channels), and downsample using stride=2\n",
    "    self.conv4direct=nn.Sequential(\n",
    "        nn.Conv2d(128,256,1,stride=2),\n",
    "        nn.BatchNorm2d(256)\n",
    "    )\n",
    "\n",
    "# Fifth block (Residual + Direct Path)\n",
    "\n",
    "    self.conv5_residual=nn.Sequential(\n",
    "        depthwiseseperableconv2d(256,256,3,padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        depthwiseseperableconv2d(256,728,3,padding=1),\n",
    "        nn.BatchNorm2d(728),\n",
    "        nn.MaxPool2d(3,stride=2,padding=1)\n",
    "    )\n",
    "    \n",
    "# Direct path: 1x1 convolution to match the output of residual (input 256 channels, output 728 channels), and downsample using stride=2\n",
    "    self.conv5direct=nn.Sequential(\n",
    "        nn.Conv2d(256,728,1,stride=2),\n",
    "        nn.BatchNorm2d(728)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    x=self.conv1(x)\n",
    "    x=self.conv2(x)\n",
    "\n",
    "    residual=self.conv3_residual(x)\n",
    "    direct=self.conv3direct(x)\n",
    "    x=residual+direct\n",
    "\n",
    "    residual=self.conv4_residual(x)\n",
    "    direct=self.conv4direct(x)\n",
    "    x=residual+direct\n",
    "\n",
    "    residual=self.conv5_residual(x)\n",
    "    direct=self.conv5direct(x)\n",
    "    x=residual+direct\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class middleblock(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(middleblock,self).__init__()\n",
    "\n",
    "    self.conv1=nn.Sequential(\n",
    "        nn.LeakyReLU(0.2),\n",
    "        depthwiseseperableconv2d(728,728,3,padding=1),\n",
    "        nn.BatchNorm2d(728),\n",
    "    )\n",
    "\n",
    "    self.conv2=nn.Sequential(\n",
    "        nn.LeakyReLU(0.2),\n",
    "        depthwiseseperableconv2d(728,728,3,padding=1),\n",
    "        nn.BatchNorm2d(728),\n",
    "    )\n",
    "    self.conv3=nn.Sequential(\n",
    "        nn.LeakyReLU(0.2),\n",
    "        depthwiseseperableconv2d(728,728,3,padding=1),\n",
    "        nn.BatchNorm2d(728),\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    residual=self.conv1(x)\n",
    "    residual=self.conv2(residual)\n",
    "    residual=self.conv3(residual)\n",
    "\n",
    "    return x+residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class middleiterativeblock(nn.Module):\n",
    "  def __init__(self,numblocks):\n",
    "    super(middleiterativeblock,self).__init__()\n",
    "    \n",
    "    ''' By using *, you are unpacking the list created by the list comprehension ([middleblock() for _ in range(numblocks)])\n",
    "    so that each element of the list is passed as an individual argument to nn.Sequential().'''\n",
    "\n",
    "    self.block=nn.Sequential(*[middleblock() for _ in range(numblocks)])\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class exitblock(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(exitblock,self).__init__()\n",
    "\n",
    "# Residual path: Depthwise separable convolutions followed by BatchNorm, LeakyReLU, and max pooling\n",
    "    self.residualconv= nn.Sequential(\n",
    "        nn.LeakyReLU(0.2),\n",
    "        depthwiseseperableconv2d(728,728,3,padding=1),\n",
    "        nn.BatchNorm2d(728),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        depthwiseseperableconv2d(728,1024,3,padding=1),\n",
    "        nn.BatchNorm2d(1024),\n",
    "        nn.MaxPool2d(3,stride=2,padding=1)\n",
    "    )\n",
    "\n",
    "# Direct path: 1x1 convolution to match the number of channels and downsample (stride=2)\n",
    "    self.direct=nn.Sequential(\n",
    "        nn.Conv2d(728,1024,1,stride=2),\n",
    "        nn.BatchNorm2d(1024)\n",
    "    )\n",
    "# Convolution block after residual and direct paths are combined\n",
    "    self.conv=nn.Sequential(\n",
    "        depthwiseseperableconv2d(1024,1536,3,padding=1),\n",
    "        nn.BatchNorm2d(1536),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        depthwiseseperableconv2d(1536,2048,3,padding=1),\n",
    "        nn.BatchNorm2d(2048),\n",
    "        nn.LeakyReLU(0.2),\n",
    "    )\n",
    "# Dropout layer for regularization to prevent overfitting    \n",
    "    self.dropout =nn.Dropout(0.3)\n",
    "\n",
    "# Global average pooling to reduce each channel to a single value (1x1 output per channel)    \n",
    "    self.globlavgpool=nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "      direct =self.direct(x)\n",
    "      residual=self.residualconv(x)\n",
    "      x=residual+direct\n",
    "      x=self.conv(x)\n",
    "      x=self.globlavgpool(x)\n",
    "      x=self.dropout(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "  def __init__(self,num_of_blocks):\n",
    "    super(Network,self).__init__()\n",
    "# Entry block: Initial layers to extract low-level features\n",
    "    self.entry_block =entryblock()\n",
    "# Middle block: A stack of `num_of_blocks` residual blocks (iterative layers) for deep feature extraction\n",
    "    self.middle_block=middleiterativeblock(num_of_blocks)\n",
    "# Exit block: Final layers to extract high-level features and reduce spatial dimensions    \n",
    "    self.exit_block=exitblock()\n",
    "\n",
    "# Linear layer that outputs 136 values (68 keypoint pairs)\n",
    "\n",
    "    self.fc=nn.Linear(2048,136)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x=self.entry_block(x)\n",
    "    x=self.middle_block(x)\n",
    "    x=self.exit_block(x)\n",
    "    x=x.view(x.size(0),-1)\n",
    "    x=self.fc(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Network(8)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model,(1,128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DEFINING THE LOSS FUNCTION AND OPTIMIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective =nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.00075)\n",
    "checkpoint_dir = 'checkpoints'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **VALIDATE FUNCTION TO EVALUATE THE PERFORMANCE OF THE MODEL** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The validate function is used to\n",
    " evaluate the performance of model on a validation dataset.'''\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(epoch):\n",
    "  tloss=0.0\n",
    "  model.eval()\n",
    "\n",
    "  for feature,labels in tqdm(val_data,desc='validating',ncols=600):\n",
    "    feature=feature.cuda()\n",
    "    labels=labels.cuda()\n",
    "\n",
    "    output=model(feature)\n",
    "    loss=objective(output,labels)\n",
    "  \n",
    "    tloss+=loss.item()\n",
    "  print(f\"The loss for the epoch {epoch}: {tloss/len(val_data)}\")\n",
    "  visiualizebatch(feature[:4].cpu(),output[:4].cpu(),shape=(1,4),size=16)\n",
    "  return tloss/len(val_data)\n",
    "\n",
    "\n",
    "validate(epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TRAINING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=30\n",
    "batches=len(train_data)\n",
    "updatedloss=np.inf\n",
    "trainingloss=[]\n",
    "validationloss=[]\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path, loss, best=False):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    if best:\n",
    "        torch.save(state, f'checkpoints/best_model.pt')  # Save the best model\n",
    "    else:\n",
    "        torch.save(state, f'checkpoints/checkpoint_everyepoch.pt')  # Save at each epoch\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  tloss=0.0\n",
    "  model.train()\n",
    "  for feature,labels in tqdm(train_data,desc =f'epoch({epoch+1}/{epochs})',ncols=800):\n",
    "    feature=feature.cuda()\n",
    "    labels=labels.cuda()\n",
    "\n",
    "    output=model(feature)\n",
    "    loss=objective(output,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    tloss+=loss.item()\n",
    "\n",
    "    '''the data goes into the validate() function because the validation dataset (val_data) is already available\n",
    "    globally or is predefined before training starts. The validate() function uses this dataset directly to perform the validation step.'''\n",
    "    \n",
    "  val_loss=validate(epoch) #used to save or log some information during validation.\n",
    "\n",
    "  save_checkpoint(model, optimizer, epoch + 1, \"checkpoints\", val_loss)\n",
    "  if val_loss<updatedloss:\n",
    "    updatedloss=val_loss\n",
    "    save_checkpoint(model, optimizer, epoch + 1, \"checkpoints\", val_loss, best=True)#Saves the model's parameters (weights) to a file named model.pt if the current model has the best validation performance so far.\n",
    "\n",
    "    '''This allows you to save the model’s progress periodically, which is especially useful if\n",
    "     you are training for a long time and don’t want to lose progress in case of a system crash.\n",
    "     By saving the model with the best validation loss,\n",
    "    you can prevent overfitting by restoring the model from an earlier point before it started overfitting.\n",
    "    '''\n",
    "  print(f'epoch({epoch+1}/{epochs}) -> training Loss:{(tloss/batches):.8f} | validation loss:{val_loss:.8f}')\n",
    "  trainingloss.append(tloss/batches)\n",
    "  validationloss.append(val_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch=np.arange(1,31)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch,trainingloss)\n",
    "plt.title(\"EPOCH VS TRAINING LOSS\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch,validationloss)\n",
    "plt.title(\"EPOCH VS VALIDATION LOSS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **FUNCTION TO LOAD CHECKPOINTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, path, best=False):\n",
    "    # Choose the best model or the last saved model\n",
    "    if best:\n",
    "        checkpoint = torch.load(f'{path}/best_model.pt')  # Load the best model\n",
    "    else:\n",
    "        checkpoint = torch.load(f'{path}/checkpoint_everyepoch.pt')  # Load the last saved checkpoint\n",
    "\n",
    "    # Load model and optimizer states\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    # Load the epoch and loss to continue training from the saved point\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    \n",
    "    print(f\"Checkpoint loaded: Resuming from epoch {start_epoch} with loss {loss}\")\n",
    "    \n",
    "    return model, optimizer, start_epoch, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **EVALUATING THE MODEL ON THE TESTSET**\n",
    "-----  INCLUDED FUNCTIONS\n",
    "- PREPROCESSING\n",
    "- DRAWING LANDMARKS ON THE ORIGINAL IMAGE\n",
    "- PREDICTING THE LANDMARKS AND DRAWING\n",
    "- FINALLY STORING THE IMAGES HAVING PREDICTED LANDMARKS DRAWN ON THEM \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the testing the model on the test set\n",
    "class datasetlandmarkeval(Dataset):\n",
    "    def __init__(self, rootdir, train):\n",
    "        self.rootdir = rootdir\n",
    "        self.imagepath = []\n",
    "        self.landmarks = []\n",
    "        self.crops_coordinates = []\n",
    "        self.train = train\n",
    "\n",
    "        # Parse the XML file to extract the image paths, landmarks, and crop coordinates\n",
    "        tree = ET.parse(os.path.join(self.rootdir, f'labels_ibug_300W_{\"train\" if train else \"test\"}.xml'))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for child in root[2]:\n",
    "            self.imagepath.append(os.path.join(self.rootdir, child.attrib['file']))\n",
    "            self.crops_coordinates.append(child[0].attrib)\n",
    "            landmark = []\n",
    "            for i in range(68):\n",
    "                x_coordinate = int(child[0][i].attrib['x'])\n",
    "                y_coordinate = int(child[0][i].attrib['y'])\n",
    "                landmark.append([x_coordinate, y_coordinate])\n",
    "            self.landmarks.append(landmark)\n",
    "\n",
    "        self.landmarks = np.array(self.landmarks).astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagepath)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load the image as a color image (default behavior for skimage.io.imread without as_gray=True)\n",
    "        image = io.imread(self.imagepath[index], as_gray=False)\n",
    "        landmarks = self.landmarks[index]\n",
    "        crops_coordinates = self.crops_coordinates[index]\n",
    "\n",
    "        # Return the raw image, landmarks, and crop coordinates without any preprocessing\n",
    "        return image, landmarks, crops_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the images for the testing of the model\n",
    "def preprocess(image):\n",
    "    image=TF.to_pil_image(image)\n",
    "    image=TF.resize(image,(128,128))\n",
    "    image=TF.to_tensor(image)\n",
    "    image=(image-image.min())/(image.max()-image.min())\n",
    "    image=(2*image)-1\n",
    "    return image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for drawing the predicted landmarks on the original image \n",
    "def draw(image,facelandmarks):\n",
    "    image = np.array(image)\n",
    "    for landmarks,(left,top,height,width) in facelandmarks:\n",
    "        landmarks=landmarks.view(-1,2) +0.5\n",
    "        landmarks =landmarks.numpy()\n",
    "        for(x,y) in landmarks:\n",
    "            if np.isnan(x) or np.isnan(y):\n",
    "                print(\"NaN value found in landmark coordinates, skipping...\")\n",
    "                continue\n",
    "            cv2.circle(image,(int((x*width)+left),int((y*height)+top)),4,[229, 245, 10],-1)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "testeval=datasetlandmarkeval(r'C:\\Users\\soham\\OneDrive\\Documents\\ALL TASK OF FACIAL LANDMARK PROJECT\\Facial_Landmark_Detection_project\\archive (1)\\ibug_300W_large_face_landmark_dataset',False)\n",
    "testeval_data=DataLoader(testeval,batch_size=1,shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting and drawing the landmarks on the images\n",
    "def predictions(image, cropcordinates):\n",
    "    x, y, w, h = cropcordinates['left'], cropcordinates['top'], cropcordinates['width'], cropcordinates['height']\n",
    "\n",
    "    \n",
    "\n",
    "    # Ensure these values are integers\n",
    "    x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "    image=TF.to_pil_image(image)\n",
    "    gray=TF.to_grayscale(image)\n",
    "    gray = np.array(gray)\n",
    "    crop = gray[y:y+h, x:x+w]\n",
    "    \n",
    "    output = []\n",
    "    preproimage = preprocess(crop)\n",
    "    \n",
    "\n",
    "    predicted=model(preproimage.cuda())\n",
    "    output.append((predicted.cpu(),(x,y,h,w)))\n",
    "    return draw(image,output)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing loop which will iterate on the each images and stored the output in the list after drawing the landmarks\n",
    "model=Network(8)\n",
    "model.cuda()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.00075)\n",
    "\n",
    "#LOADING THE CHECKPOINTS\n",
    "model, optimizer, start_epoch, updatedloss = load_checkpoint(model, optimizer, \"checkpoints\", best=True)\n",
    "@torch.no_grad()\n",
    "def validatetest():\n",
    "  outputs=[]\n",
    "  model.eval()\n",
    "\n",
    "  for image,landmarks,cropcordinates in tqdm(testeval,desc='validating',ncols=600):\n",
    "    output=predictions(image,cropcordinates)\n",
    "    \n",
    "    outputs.append(output)\n",
    "  \n",
    "   \n",
    "  \n",
    " \n",
    "  return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=validatetest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i in range(0,12):\n",
    "    plt.subplot(3,4,i+1)\n",
    "    plt.imshow(output[800+i])\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
