{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the code for converting the ubyte file to the csv files \n",
    "def convert(img,label,outfile,n):\n",
    "  imgf =open(img,\"rb\")\n",
    "  labelf=open(label,\"rb\")\n",
    "  csvf=open(outfile,\"w\")\n",
    "  imgf.read(16)\n",
    "  labelf.read(8)\n",
    "  images=[]\n",
    "  for i in range(n):\n",
    "    image=[ord(labelf.read(1))]\n",
    "    for j in range(28*28):\n",
    "      image.append(ord(imgf.read(1)))\n",
    "    images.append(image)\n",
    "  for image in images:\n",
    "    csvf.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n",
    "  imgf.close()\n",
    "  csvf.close()\n",
    "  labelf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data from the directory\n",
    "trainx=r'C:\\Users\\soham\\OneDrive\\Documents\\ALL TASK OF FACIAL LANDMARK PROJECT\\task_1_neutralN_from _scratch\\archive (2)\\train-images.idx3-ubyte'\n",
    "trainy=r'C:\\Users\\soham\\OneDrive\\Documents\\ALL TASK OF FACIAL LANDMARK PROJECT\\task_1_neutralN_from _scratch\\archive (2)\\train-labels.idx1-ubyte'\n",
    "testy=r'C:\\Users\\soham\\OneDrive\\Documents\\ALL TASK OF FACIAL LANDMARK PROJECT\\task_1_neutralN_from _scratch\\archive (2)\\t10k-labels.idx1-ubyte'\n",
    "testx=r'C:\\Users\\soham\\OneDrive\\Documents\\ALL TASK OF FACIAL LANDMARK PROJECT\\task_1_neutralN_from _scratch\\archive (2)\\t10k-images.idx3-ubyte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the covert function\n",
    "convert(trainx,trainy,r'C:\\Users\\soham\\OneDrive\\Documents\\ALL TASK OF FACIAL LANDMARK PROJECT\\task_1_neutralN_from _scratch\\archive (2)\\train.csv',60000)\n",
    "convert(testx,testy,r'C:\\Users\\soham\\OneDrive\\Documents\\ALL TASK OF FACIAL LANDMARK PROJECT\\task_1_neutralN_from _scratch\\archive (2)\\test.csv',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code sets up the data for a neural network by reading the CSV files, shuffling, splitting into training and validation sets, normalizing the data,\n",
    "#and preparing the feature matrices Xtrain, Xdev and label vectors Ytrain, Ydev.\n",
    "trainset=pd.read_csv(r'C:\\Users\\soham\\OneDrive\\Documents\\ALL TASK OF FACIAL LANDMARK PROJECT\\task_1_neutralN_from _scratch\\archive (2)\\train.csv')\n",
    "testset=pd.read_csv(r'C:\\Users\\soham\\OneDrive\\Documents\\ALL TASK OF FACIAL LANDMARK PROJECT\\task_1_neutralN_from _scratch\\archive (2)\\test.csv')\n",
    "trainset=np.array(trainset)\n",
    "m,n=trainset.shape\n",
    "np.random.shuffle(trainset)\n",
    "datadev=trainset[0:1000].T\n",
    "Ydev=datadev[0]\n",
    "Xdev=datadev[1:n]\n",
    "Xdev=Xdev / 255.\n",
    "datatrain=trainset[1000:m].T\n",
    "Ytrain=datatrain[0]\n",
    "Xtrain=datatrain[1:n]\n",
    "Xtrain=Xtrain / 255.\n",
    "_,m_train=Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    # Create a matrix of zeros with shape (number of examples, number of unique classes)\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    # Set the appropriate index in each row to 1, based on the class label\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainydata=one_hot(Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the parameter for the model like layers,weights, biases\n",
    "def initializepara(layer):\n",
    "  parameter={}\n",
    "  l=len(layer)\n",
    "  for i in range(1,l):\n",
    "    parameter[\"W\"+str(i)]=np.random.randn(layer[i],layer[i-1])*0.01\n",
    "    parameter[\"b\"+str(i)]=np.zeros((layer[i],1))\n",
    "  return parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the sigmoid function\n",
    "def sigmoid(Z):\n",
    "  A=1/(1+np.exp(-Z))\n",
    "  cache=Z\n",
    "  return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the relu function\n",
    "def relu(Z):\n",
    "  A=np.maximum(0,Z)\n",
    "  cache=Z\n",
    "  return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the softmax function\n",
    "def softmax(Z):\n",
    "  A=np.exp(Z)/np.sum(np.exp(Z),axis=0,keepdims=True)\n",
    "  cache=Z\n",
    "  return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implements the linear part of forward propagation for a single layer.\n",
    "\n",
    "def linearforward(A,W,b):\n",
    "  z=np.dot(W,A)+b\n",
    "  cache=(A,W,b)\n",
    "  return z,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function abstracts the choice of activation function for the layer, enabling the network to handle different activation types efficiently.\n",
    "def forwardactivation(Aprev,W,b,activation):\n",
    "  if activation==\"sigmoid\":\n",
    "    Z,lincache=linearforward(Aprev,W,b)\n",
    "    A,actcache=sigmoid(Z)\n",
    "  elif activation==\"relu\":\n",
    "    Z,lincache=linearforward(Aprev,W,b)\n",
    "    A,actcache=relu(Z)\n",
    "  elif activation==\"softmax\":\n",
    "    Z,lincache=linearforward(Aprev,W,b)\n",
    "    A,actcache=softmax(Z)\n",
    "  cache=(lincache,actcache)\n",
    "  return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implements forward propagation for the entire network.\n",
    "\n",
    "def forwardprop(X,parameters):\n",
    "  caches=[]  # List to store caches for each layer\n",
    "  A=X    # Set the input data as the activation of the 0th layer\n",
    "  l=len(parameters)//2  #Number of layers (each layer has W and b)\n",
    "  for i in range(1,l):\n",
    "    Aprev=A\n",
    "    A,cache=forwardactivation(Aprev,parameters[\"W\"+str(i)],parameters[\"b\"+str(i)],\"relu\")\n",
    "    caches.append(cache)\n",
    "  # Compute the output of the final layer using softmax activation  \n",
    "  AL,cache=forwardactivation(A,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],\"softmax\")\n",
    "  caches.append(cache)\n",
    "  return AL,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The costcompute function calculates the cross-entropy cost for a neural network\n",
    "def costcompute(AL,Y):\n",
    "  m=Y.shape[1]\n",
    "  cost=-np.sum(Y*np.log(AL))/m\n",
    "  cost=np.squeeze(cost)\n",
    "  return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implements the linear portion of backward propagation for a single layer.\n",
    "#computing the gradient of the cost function with respect to the weight ,biases,input.\n",
    "def linearbackward(dZ,cache):\n",
    "  Aprev,W,b=cache\n",
    "  m=Aprev.shape[1]\n",
    "  dW=np.dot(dZ,Aprev.T)/m\n",
    "  db=np.sum(dZ, axis=1, keepdims=True) / m\n",
    "  dAprev=np.dot(W.T,dZ)\n",
    "  return dAprev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the reludrivative function\n",
    "def Reluderiv(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implements the backward propagation for the entire network.\n",
    "def backwardprop(AL,Y,caches):\n",
    "  grads={}\n",
    "  l=len(caches)\n",
    "  m=AL.shape[1]\n",
    "  Y=Y.reshape(AL.shape)\n",
    "  # Compute the gradient of the cost with respect to AL (softmax output)\n",
    "  dAL=AL-Y\n",
    "\n",
    "  # Backpropagation for the last layer (softmax + linear)\n",
    "  currentcache=caches[-1]\n",
    "  grads[\"dA\"+str(l)],grads[\"dW\"+str(l)],grads[\"db\"+str(l)]=linearbackward(dAL,currentcache[0])\n",
    "  # Loop over all hidden layers in reverse order, using ReLU derivative\n",
    "  for i in reversed(range(l-1)):\n",
    "    currentcache=caches[i]\n",
    "    dAprev,dW,db=linearbackward(grads[\"dA\"+str(i+2)]*Reluderiv(currentcache[1]),currentcache[0])\n",
    "    grads[\"dA\"+str(i+1)]=dAprev\n",
    "    grads[\"dW\"+str(i+1)]=dW\n",
    "    grads[\"db\"+str(i+1)]=db\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializes the velocity terms for gradient descent with momentum optimization algo.\n",
    "def initializevelocity(parameters):\n",
    "  l=len(parameters)//2\n",
    "  v={}\n",
    "  for i in range(1,l+1):\n",
    "    v[\"dw\"+str(i)]=np.zeros(parameters[\"W\"+str(i)].shape)\n",
    "    v[\"db\"+str(i)]=np.zeros(parameters[\"b\"+str(i)].shape)\n",
    "  return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for implementing gradient descent with momentum algo \n",
    "def upadatewithmomentum(parameters,grads,v,beta,learningrate):\n",
    "  l=len(parameters)//2\n",
    "  for i in range(1,l+1):\n",
    "    v[\"dw\"+str(i)]=beta*v[\"dw\"+str(i)]+(1-beta)*grads[\"dW\"+str(i)]\n",
    "    v[\"db\" +str(i)]=beta*v[\"db\"+str(i)]+(1-beta)*grads[\"db\"+str(i)]\n",
    "    parameters[\"W\"+str(i)]=parameters[\"W\"+str(i)]-learningrate*v[\"dw\"+str(i)]\n",
    "    parameters[\"b\"+str(i)]=parameters[\"b\"+str(i)]-learningrate*v[\"db\"+str(i)]\n",
    "  return parameters,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function for updating the parameters\n",
    "def updateparameters(parameters,grads,learningrate):\n",
    "  l=len(parameters)//2\n",
    "  for i in range(1,l+1):\n",
    "    parameters[\"W\"+str(i)]=parameters[\"W\"+str(i)]-learningrate*grads[\"dW\"+str(i)]\n",
    "    parameters[\"b\"+str(i)]=parameters[\"b\"+str(i)]-learningrate*grads[\"db\"+str(i)]\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    #Uses np.argmax to find the index of the maximum value\n",
    "    #Returns an array of predicted class labels\n",
    "    return np.argmax(A2, 0)\n",
    "#defining the funtion for the accuracy calculations\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llayermodel(X, Y, layer, learningrate=0.0075, numiterations=3000, printcost=False):\n",
    "  costs=[]\n",
    "  parameters=initializepara(layer)\n",
    "  for i in range(numiterations):\n",
    "    # Forward propagation\n",
    "    AL,caches=forwardprop(X,parameters)\n",
    "    # Compute cost\n",
    "    cost=costcompute(AL,Y)\n",
    "    # Backward propagation\n",
    "    grads=backwardprop(AL,Y,caches)\n",
    "    # Update parameters\n",
    "    parameters=updateparameters(parameters,grads,learningrate)\n",
    "    # Print cost every 100 iterations if specified\n",
    "    if printcost and i%100==0:\n",
    "      print(\"cost after iteration %i: %f\" %(i,cost))\n",
    "    if printcost and i%100==0:\n",
    "     costs.append(cost)\n",
    "  #plot the cost over iterations   \n",
    "  plt.plot(np.squeeze(costs))\n",
    "  plt.ylabel(\"cost\")\n",
    "  plt.xlabel(\"iterations \")\n",
    "  plt.title(\"learning rate:\"+str(learningrate))\n",
    "  plt.show()\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=llayermodel(Xtrain,trainydata,[784,10,10],0.1,3000,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL,cache=forwardprop(Xtrain,parameters)\n",
    "t=get_accuracy(get_predictions(AL),Ytrain)\n",
    "print(\"Accuracy: \" + str(t*100)+\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
